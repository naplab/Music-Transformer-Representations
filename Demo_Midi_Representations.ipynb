{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64073049",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b82119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('musicautobot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26932043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musicautobot.numpy_encode import *\n",
    "from musicautobot.utils.file_processing import process_all, process_file\n",
    "from musicautobot.config import *\n",
    "from musicautobot.music_transformer import *\n",
    "from musicautobot.multitask_transformer import *\n",
    "from musicautobot.numpy_encode import stream2npenc_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae63da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import environment\n",
    "# environment.set(\"musescoreDirectPNGPath\", \"/usr/bin/musescore3\")\n",
    "environment.set(\"musescoreDirectPNGPath\", \"/Applications/MuseScore 3.app/Contents/MacOS/mscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c992ced6-d452-4185-8c88-0f96cd435abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/numpy/pretrained', exist_ok=True)\n",
    "os.makedirs('../data/midi', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88c2065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Config\n",
    "config = multitask_config();\n",
    "\n",
    "# Location of your midi files\n",
    "midi_path =  Path('../data/midi')\n",
    "\n",
    "# Location of saved datset\n",
    "data_path = Path('../data/numpy')\n",
    "data_save_name = 'musicitem_data_save.pkl'\n",
    "\n",
    "# Data\n",
    "data = MusicDataBunch.empty(data_path)\n",
    "vocab = data.vocab\n",
    "\n",
    "# Download pretrained model if you haven't already\n",
    "pretrained_url = 'https://ashaw-midi-web-server.s3-us-west-2.amazonaws.com/pretrained/MultitaskLargeKeyC.pth'\n",
    "\n",
    "pretrained_path = data_path/'pretrained'/Path(pretrained_url).name\n",
    "download_url(pretrained_url, dest=pretrained_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d874892-94c6-4cd6-98c3-1d09023350e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner\n",
    "learn = multitask_model_learner(data, pretrained_path=pretrained_path)\n",
    "# learn.to_fp16();\n",
    "\n",
    "learn.model = learn.model.eval()\n",
    "learn.model.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a826c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3c2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn.manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71543873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# mypath = \"../suprisial_stimuli/transformer_activation\"\n",
    "onlyfiles = sorted([f for f in listdir(midi_path) if isfile(join(midi_path, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "10f3b604-5074-4a4a-a5b4-933436644811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lm_mask(x_len, device='cpu'):\n",
    "    mask = torch.triu(torch.ones((x_len, x_len), device=device), diagonal=1)[None]\n",
    "    return mask.bool() if hasattr(mask, 'bool') else mask.byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "55e47522-8773-4a09-9bf0-2b601157374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio1.mid\n",
      "torch.Size([1, 4086, 4086])\n",
      "audio10.mid\n",
      "torch.Size([1, 2570, 2570])\n",
      "audio2.mid\n",
      "torch.Size([1, 3562, 3562])\n",
      "audio3.mid\n",
      "torch.Size([1, 1204, 1204])\n",
      "audio4.mid\n",
      "torch.Size([1, 2118, 2118])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gavin/miniconda3/envs/musicautobot/lib/python3.9/site-packages/music21/midi/translate.py:863: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=5, channel=None, data=b'4th Movement: bour\\xe9e'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio5.mid\n",
      "torch.Size([1, 2046, 2046])\n",
      "audio6.mid\n",
      "torch.Size([1, 6416, 6416])\n",
      "audio7.mid\n",
      "torch.Size([1, 2168, 2168])\n",
      "audio8.mid\n",
      "torch.Size([1, 5418, 5418])\n",
      "audio9.mid\n",
      "torch.Size([1, 1354, 1354])\n"
     ]
    }
   ],
   "source": [
    "embeddings_byfile = {}\n",
    "\n",
    "for f in onlyfiles:\n",
    "    print(f)\n",
    "    embeddings = []\n",
    "    item = MusicItem.from_file(os.path.join(midi_path, f), data.vocab)\n",
    "    x = item.to_tensor().unsqueeze(0)\n",
    "    x_pos = item.get_pos_tensor().unsqueeze(0)\n",
    "    \n",
    "    bs, lm_len = x.size()\n",
    "    lm_mask = get_lm_mask(lm_len)\n",
    "    msk_emb = None\n",
    "            \n",
    "    lm_emb = learn.model.encoder.embed(x, x_pos)\n",
    "    if msk_emb is not None and msk_emb.shape[1] > lm_emb.shape[1]:\n",
    "        pos_enc = learn.model.encoder.embed.relative_pos_enc(msk_emb)\n",
    "    else:\n",
    "        pos_enc = learn.model.encoder.embed.relative_pos_enc(lm_emb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    layer_embeddings = []\n",
    "    for i, layer in enumerate(learn.model.encoder.layers):\n",
    "        \n",
    "        lm_emb = layer(lm_emb, msk_emb, lm_mask=lm_mask,\n",
    "                    r=pos_enc, g_u=learn.model.encoder.u, g_v=learn.model.encoder.v)\n",
    "        layer_embeddings.append(lm_emb)\n",
    "        \n",
    "    layer_embeddings = torch.stack(layer_embeddings).squeeze(1)\n",
    "\n",
    "    embeddings_byfile[f] = layer_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "469f849d-cc93-496d-9ee3-954af0c0aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('audio1.mid', torch.Size([12, 4086, 768]))\n",
      "('audio10.mid', torch.Size([12, 2570, 768]))\n",
      "('audio2.mid', torch.Size([12, 3562, 768]))\n",
      "('audio3.mid', torch.Size([12, 1204, 768]))\n",
      "('audio4.mid', torch.Size([12, 2118, 768]))\n",
      "('audio5.mid', torch.Size([12, 2046, 768]))\n",
      "('audio6.mid', torch.Size([12, 6416, 768]))\n",
      "('audio7.mid', torch.Size([12, 2168, 768]))\n",
      "('audio8.mid', torch.Size([12, 5418, 768]))\n",
      "('audio9.mid', torch.Size([12, 1354, 768]))\n"
     ]
    }
   ],
   "source": [
    "for k, v in embeddings_byfile.items():\n",
    "    print((k, v.shape)) # each is shape (layers, notes, hidden_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc46ec-8cc4-4baf-b942-e91dc2543309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicautobot",
   "language": "python",
   "name": "musicautobot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
